{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning 2- Mountain Car Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the continuation of [previous one](https://github.com/sezan92/DQL/blob/master/Deep%20Q%20Learning.ipynb) . Please check That code before starting this one. In this notebook I am trying to solve Mountain Car problem by ```Open AI```. The video is available [here](https://www.youtube.com/watch?v=MbArDXXYcjM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rule of the game is simple. The car at the valley must reach the flag at the right top, with the least steps possible. For each step it will get -1 reward. The game is considered won when the car gets -110 . The game is considered failure when the car takes 200 steps but didn't reach the flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mountain car](Final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the parts of the codes are same. So I will try to explain which parts are slightly different , and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2018-03-06 23:43:59,858] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "# In[2]\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_actions=env.action_space.n\n",
    "actions = [0,1]\n",
    "gamma =0.95\n",
    "lr =0.5\n",
    "num_episodes =1000\n",
    "epsilon =1\n",
    "epsilon_decay =0.995\n",
    "memory_size =1000\n",
    "batch_size=100\n",
    "show=False\n",
    "action_size=env.action_space.shape[0]\n",
    "state_size=env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change ```show``` variable to ```True``` to visualize the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at this part from Open Ai gym's wiki [page](https://github.com/openai/gym/wiki/MountainCar-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MountainObs](MountainCarObs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum velocity is -0.07 units and the maximum is 0.07 units. While the minimum position is -1.2 units and maximum position is 0.6 units. Here is the problem . The scale of the data is not the same. It is best to scale the data. For the position values, the scale is good. It is within 0 to 1. But the velocity observations' scale is pretty bad. Machine Learning Algorithms usually don't work good in those scales. So what's the solution ? Simple, multiplying different factors to the states before working with it. I have defined a list named $factor$ which will rescale the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factor=[1,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different values for the factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code is almost same , but there is a slight difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[4]\n",
    "if ep_list is None and reward_list is None:\n",
    "    ep_list =[]\n",
    "    reward_list =[] \n",
    "index=0 \n",
    "for ep in range(num_episodes):\n",
    "    s= env.reset()\n",
    "    s=s.reshape((1,-1))\n",
    "    s = s*factor\n",
    "    rAll =0\n",
    "    d = False\n",
    "    j = 0\n",
    "    for j in range(200):\n",
    "        #time.sleep(0.01)\n",
    "        #epsilon greedy. to choose random actions initially when Q is all zeros\n",
    "        if np.random.random()< epsilon:\n",
    "            a = np.random.randint(0,legal_actions)\n",
    "            #epsilon = epsilon*epsilon_decay\n",
    "        else:\n",
    "            Q = model.predict(s.reshape(-1,s.shape[0],s.shape[1]))\n",
    "            a =np.argmax(Q)\n",
    "        new_s,r,d,_ = env.step(a)\n",
    "        new_s = new_s.reshape((1,-1))\n",
    "        new_s = new_s*factor\n",
    "        rAll=rAll+r\n",
    "        if show:\n",
    "            env.render()\n",
    "        if d:\n",
    "            if rAll<-199:\n",
    "                r =-100\n",
    "                experience = (s,r,a,new_s)\n",
    "                memory.append(experience)\n",
    "                print(\"Episode %d, Failed! Reward %d\"%(ep,rAll))\n",
    "                #break\n",
    "            elif rAll<-110 and rAll>-199:\n",
    "                r=-10\n",
    "                experience = (s,r,a,new_s)\n",
    "                memory.append(experience)\n",
    "                print(\"Episode %d, Better! Reward %d\"%(ep,rAll))\n",
    "            elif rAll>=-110:\n",
    "                r=100\n",
    "                experience = (s,r,a,new_s)\n",
    "                memory.append(experience)\n",
    "\n",
    "                print(\"Episode %d, Passed! Reward %d\"%(ep,rAll))\n",
    "            ep_list.append(ep)\n",
    "            reward_list.append(rAll)\n",
    "            break\n",
    "        \n",
    "        experience = (s,r,a,new_s)\n",
    "        memory.append(experience)\n",
    "        if j==199:\n",
    "            print(\"Reward %d after full episode\"%(rAll))\n",
    "            \n",
    "        s = new_s\n",
    "    batches=random.sample(memory,batch_size)\n",
    "    #batches= list(memory)[index:index+batch_size]\n",
    "    states= np.array([batch[0] for batch in batches])\n",
    "    rewards= np.array([batch[1] for batch in batches])\n",
    "    actions= np.array([batch[2] for batch in batches])\n",
    "    new_states= np.array([batch[3] for batch in batches])\n",
    "    Qs =model.predict(states)\n",
    "    new_Qs = model.predict(new_states)\n",
    "    for i in range(len(rewards)):\n",
    "        if rewards[i]==-100 or rewards[i]==-10:\n",
    "            Qs[i][0][actions[i]]=Qs[i][0][actions[i]]+ lr*(rewards[i]-Qs[i][0][actions[i]])\n",
    "        else:\n",
    "            Qs[i][0][actions[i]]= Qs[i][0][actions[i]]+ lr*(rewards[i]+gamma*np.max(new_Qs[i])-Qs[i][0][actions[i]])\n",
    "    model.fit(states,Qs,verbose=0)\n",
    "    epsilon=epsilon*epsilon_decay\n",
    "    index=index+batch_size\n",
    "    if index>=len(memory):\n",
    "        index=0\n",
    "env.close()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slight difference is the punishment for stopping before agent reaches the flag. Please figure it out yourself :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MountainDQL](Reward_vs_Episode_DL_lr_0.500000_eps_1000.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for Vanilla Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MountainCarVanillaQ](Reward_vs_Episode_QL_lr_0.500000_eps_1000.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the The Deep learning Based Q Network got more points and was more stable compared to Vanilla Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
